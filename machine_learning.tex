\chapter{Machine Learning}

\section{Algorithms}
In this project, we picked 8 type of classifiers. They are:
\begin{enumerate}
    \item RandomForest
        \begin{itemize}
            \item Random Forest classifier is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.
        \end{itemize}
    \item ExtraTrees
        \begin{itemize}
            \item  The main difference is that when at each node RandomForest chooses the best cutting threshold for the feature, ExtraTrees instead chooses the cut (uniformly) randomly. Similarly to RandomForest the feature with the biggest gain (or best score) is chosen after the cutting threshold has been fixed.
        \end{itemize}
    \item NaiveBayes
        \begin{itemize}
            \item Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features.
        \end{itemize}
    \item LogitBoosting
        \begin{itemize}
            \item LogitBoosting is an addictive, boosted model that seeks to minimize logistic errors.
        \end{itemize}
    \item SVM
        \begin{itemize}
            \item Support vector machines is a discriminative classifier formally defined by a separating hyperplane. The operation of the SVM algorithm is based on finding the hyperplane that gives the largest minimum distance to the training examples.
        \end{itemize}
    \item Bagging
        \begin{itemize}
            \item A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.
        \end{itemize}
    \item SGDClassifier
        \begin{itemize}
            \item Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial\_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.
        \end{itemize}
    \item NeuralNetwork
        \begin{itemize}
            \item Neural Network
        \end{itemize}
\end{enumerate}

\section{Program Flow}

The flow of the program is structured as follows:

\begin{enumerate}
    \item Converting numerical response into categorical data
        \begin{itemize}
            \item Classifiers required a class label to classify based on the input (features). Thus for all the seven output metrics, the numeric data are classified into binary class 0 or 1 with 1 representing the numeric data is greater than median. Otherwise, 0. 
        \end{itemize}
    \item Validating Models
        \begin{itemize}
            \item K-fold cross validation method is used to validate models and also to limit the overfitting problems.
        \end{itemize}
    \item Generate ROC curve and AUC charts
        \begin{itemize}
            \item ROC curve and AUC charts are then plotted to be used in evaluating the performance of the classifiers.
        \end{itemize}
\end{enumerate}